{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4263851d-4373-4695-8ac5-f5e36bb7fa60",
   "metadata": {},
   "source": [
    "# Part 1 (Sentiment analysis):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb46bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vader sentiment analysis package, transformers, pytorch\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f11b3c-ed84-4569-8d8e-828849ee32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_sent(sentence):\n",
    "    VSA = SentimentIntensityAnalyzer()\n",
    "    sentiment = VSA.polarity_scores(sentence)\n",
    "    label ='P' if sentiment['compound'] >= 0.05 else ('N' if sentiment['compound'] <= -0.05 else None) #when -0.05<compound<0.05, it is neutral\n",
    "    if label:\n",
    "        with open('Sentiment Analysis.txt','a') as f:\n",
    "            f.write(f'{sentence} {label}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ccf2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a list of 100 sentences,compare the results of Naive Bayes/Vader and the finetuned BERT model\n",
    "with open('en.txt','r',encoding='utf8') as f:\n",
    "    content = f.read()\n",
    "content = (' '.join(content.split())).replace('\\n',' ')\n",
    "sent = nltk.sent_tokenize(content)\n",
    "sample_sent = sent[1:101]\n",
    "\n",
    "# BERT Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\",model=\"distilbert-base-uncased-finetuned-sst-2-english\",device=device)\n",
    "def BERT_sentiment(text):\n",
    "    sentiment = sentiment_pipeline(text)\n",
    "    label = sentiment[0]['label']\n",
    "    return 'P' if label == 'POSITIVE' else 'N'\n",
    "\n",
    "#Vader sentiment\n",
    "def Vader_sentiment(text):\n",
    "    VSA = SentimentIntensityAnalyzer()\n",
    "    sentiment = VSA.polarity_scores(text)\n",
    "    return 'P' if sentiment['compound'] >= 0.05 else ('N' if sentiment['compound'] <= -0.05 else 'Neutral')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80d11185",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_analysis = [BERT_sentiment(sentence) for sentence in sample_sent]\n",
    "Vader_analysis = [Vader_sentiment(sentence) for sentence in sample_sent]\n",
    "with open('sent_comp.txt','a') as f:\n",
    "    f.write('Sentence, ResNB/Vader, ResBERT\\n')\n",
    "    for i in range(len(sample_sent)):\n",
    "        f.write(f'\"{sample_sent[i]}\",{Vader_analysis[i]},{BERT_analysis[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63358417-6661-4c5b-a0b6-56a04c30fc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agreement: 51\n",
      "Ratio of Agreement: 0.51\n",
      "Ratio of Agreement(Excluding Neutral Results): 0.6375\n"
     ]
    }
   ],
   "source": [
    "#Calculate how often the two models agree\n",
    "agree, total = 0, 0\n",
    "for i in range(len(sample_sent)):\n",
    "    if Vader_analysis[i] == 'Neutral':\n",
    "        continue\n",
    "    if Vader_analysis[i] == BERT_analysis[i]:\n",
    "        agree += 1\n",
    "    total += 1\n",
    "\n",
    "print(f'Number of agreement: {agree}')\n",
    "print(f'Ratio of Agreement: {agree/len(sample_sent)}')\n",
    "print(f'Ratio of Agreement(Excluding Neutral Results): {agree/total}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba0d39-338d-4677-a68b-fa249232e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check which model provide the right response\n",
    "agree_cases = []\n",
    "diff_cases = []\n",
    "for i in range(len(sample_sent)):\n",
    "    if Vader_analysis[i] == 'Neutral':\n",
    "        continue\n",
    "    if Vader_analysis[i] == BERT_analysis[i]:\n",
    "        agree_cases.append((sample_sent[i],Vader_analysis[i],BERT_analysis[i]))\n",
    "    else:\n",
    "        diff_cases.append((sample_sent[i],Vader_analysis[i],BERT_analysis[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de3a0eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples of Agree Case: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Lucy Graham was not looking at Sir Michael, but straight out into the misty twilight and dim landscape far away beyond the little garden.',\n",
       "  'P',\n",
       "  'P'),\n",
       " ('To the right there were the kitchen gardens, the fish-pond, and an orchard bordered by a dry moat, and a broken ruin of a wall, in some places thicker than it was high, and everywhere overgrown with trailing ivy, yellow stonecrop, and dark moss.',\n",
       "  'N',\n",
       "  'N'),\n",
       " ('But this reference was so satisfactory that none other was needed, and Miss Lucy Graham was received by the surgeon as the instructress of his daughters.',\n",
       "  'P',\n",
       "  'P'),\n",
       " ('What had been his love for his first wife but a poor, pitiful, smoldering spark, too dull to be extinguished, too feeble to burn?',\n",
       "  'N',\n",
       "  'N'),\n",
       " ('Often in the cool of the evening Sir Michael Audley would stroll up and down smoking his cigar, with his dogs at his heels, and his pretty young wife dawdling by his side; but in about ten minutes the baronet and his companion would grow tired of the rustling limes and the still water, hidden under the spreading leaves of the water-lilies, and the long green vista with the broken well at the end, and would stroll back to the drawing-room, where my lady played dreamy melodies by Beethoven and Mendelssohn till her husband fell asleep in his easy-chair.',\n",
       "  'N',\n",
       "  'N')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "print('Samples of Agree Case: ')\n",
    "random.sample(agree_cases,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f62e65a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples of Different Judgement Case: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The truth was that Lady Audley had, in becoming the wife of Sir Michael, made one of those apparently advantageous matches which are apt to draw upon a woman the envy and hatred of her sex.',\n",
       "  'N',\n",
       "  'P'),\n",
       " ('\"I scarcely think there is a greater sin, Lucy,\" he said, solemnly, \"than that of a woman who marries a man she does not love.',\n",
       "  'N',\n",
       "  'P'),\n",
       " ('It pained him too much to believe for a moment that any one so lovely and innocent could value herself against a splendid house or a good old title.',\n",
       "  'P',\n",
       "  'N'),\n",
       " ('\"You unlucky, my dear!\"', 'P', 'N'),\n",
       " ('There was nothing whatever in her manner that betrayed the shallow artifices employed by a woman who wishes to captivate a rich man.',\n",
       "  'P',\n",
       "  'N')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Samples of Different Judgement Case: ')\n",
    "random.sample(diff_cases,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126c517f-89aa-4fb0-ad6b-929d3b4b0301",
   "metadata": {},
   "source": [
    "##### Analysis\n",
    "1. 'Of course it would be a magnificent match; he has a splendid income, and is one of the most generous of men.'  \n",
    "    \n",
    "    When the sentence is simple and straightforward, both models make the right analysis that it is positive. The sentence is straight-forward and contains words explicitly express compliment. So it would be easy for both models to make a right judgement.\n",
    "\n",
    "Even if the models agree, it does not mean they provide the right analysis. For example, in sentence:\n",
    "\n",
    "2. 'A spot in which peace seemed to have taken up her abode, setting her soothing hand on every tree and flower, on the still ponds and quiet alleys, the shady corners of the old-fashioned rooms, the deep window-seats behind the painted glass, the low meadows and the stately avenues—ay, even upon the stagnant well, which, cool and sheltered as all else in the old place, hid itself away in a shrubbery behind the gardens, with an idle handle that was never turned and a lazy rope so rotten that the pail had broken away from it, and had fallen into the water.'  \n",
    "    \n",
    "    Both models think it is negative, but the sentiment of this sentence is positive. Both models have limitations in dealing with complex sentences in abstract or ambiguous context.\n",
    "\n",
    "3. 'Of course, in such a house there were secret chambers; the little daughter of the present owner, Sir Michael Audley, had fallen by accident upon the discovery of one.'   \n",
    "    \n",
    "    Both models think this sentence is negative. However, it is only a neutral description. They might have mistakenly associated 'secret chambers' with \n",
    "some negative associations.\n",
    "\n",
    "When the two models make a different judgement:\n",
    "\n",
    "4. ‘It had been of good service in its time, no doubt; and busy nuns have perhaps drawn the cool water with their own fair hands; but it had fallen into disuse now, and scarcely any one at Audley Court knew whether the spring had dried up or not.’  \n",
    "    \n",
    "    The Vader thinks the sentence is positive while the finetuned BERT model thinks it is negative. In this case, the finetuned BERT model makes the right judgement. The Vader model mainly relies on the emotions of each words in the sentences with semantic rules.Therefore, it may decide the sentence is positive when the sentence contains more obviously positive words in the sentence. The finetuned BERT model analyse sentiment based on the context of the whole sentence and thus identify the emotional change in the sentence.\n",
    "\n",
    "5. ‘The truth was that Lady Audley had, in becoming the wife of Sir Michael, made one of those apparently advantageous matches which are apt to draw upon a woman the envy and hatred of her sex.’\n",
    "\n",
    "    The Vader thinks the sentence is negative while the finetuned BERT model thinks it is positive. From my perspective, the Vader made the right judgement. As the sentence contains 'envy' and 'hatred', which clearly connected to strong negative emotions, thus the Vader tends to judge it as negative. The BERT model judged it as positive as it understand context and its judgement is influenced by the 'apparently advantageous matches'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac1536-1314-405e-8a4e-a3024ed19c63",
   "metadata": {},
   "source": [
    "# Part 2 (Text classification)\n",
    "perform text classification using scikit-learn\n",
    "check whether named entities can positively contribute to a better classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8abfbda3-9745-4ce2-b885-644a9ef18692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1102, 1657)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cats = ['alt.atheism', 'sci.space', 'comp.graphics']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=cats)\n",
    "\n",
    "len(newsgroups_test['data']),len(newsgroups_train['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "543cba83-c327-4332-bfd7-a090842d481b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248, 248, 165, 165)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This whole cell is just to get a smaller subset of the corpus\n",
    "\n",
    "whole_train_instances = newsgroups_train['data']\n",
    "whole_y_train = [newsgroups_train['target_names'][el] for el in newsgroups_train['target']]\n",
    "\n",
    "train_instances, _, y_train, _ = train_test_split(whole_train_instances, whole_y_train, train_size=0.15)\n",
    "\n",
    "whole_test_instances = newsgroups_test['data']\n",
    "whole_y_test = [newsgroups_test['target_names'][el] for el in newsgroups_test['target']]\n",
    "\n",
    "test_instances, _, y_test, _ = train_test_split(whole_test_instances, whole_y_test, train_size=0.15)\n",
    "\n",
    "len(train_instances), len(y_train), len(test_instances), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca63ae4-33ab-449a-a050-c609c25952f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 414, 165, 165)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get a smaller subset of the corpus, as this part can be quite computationally expensive.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "whole_train_instances = newsgroups_train['data']\n",
    "whole_y_train = [newsgroups_train['target_names'][el] for el in newsgroups_train['target']]\n",
    "\n",
    "train_instances, _, y_train, _ = train_test_split(whole_train_instances, whole_y_train, train_size=0.25)\n",
    "\n",
    "whole_test_instances = newsgroups_test['data']\n",
    "whole_y_test = [newsgroups_test['target_names'][el] for el in newsgroups_test['target']]\n",
    "\n",
    "test_instances, _, y_test, _ = train_test_split(whole_test_instances, whole_y_test, train_size=0.15)\n",
    "\n",
    "len(train_instances), len(y_train), len(test_instances), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90317e3-4b88-436c-bfa5-0692d78f2916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Classification Report-----------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  alt.atheism       0.96      0.92      0.94       319\n",
      "comp.graphics       0.92      0.96      0.94       389\n",
      "    sci.space       0.95      0.94      0.94       394\n",
      "\n",
      "     accuracy                           0.94      1102\n",
      "    macro avg       0.94      0.94      0.94      1102\n",
      " weighted avg       0.94      0.94      0.94      1102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Vectorize the news articles and train a Logistic Regression model on the training data.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "whole_train_countmatrix = vectorizer.fit_transform(whole_train_instances)\n",
    "whole_test_countmatrix = vectorizer.transform(whole_test_instances)\n",
    "\n",
    "lr_model = LogisticRegression(random_state=442).fit(whole_train_countmatrix, whole_y_train)\n",
    "y_pred = lr_model.predict(whole_test_countmatrix)\n",
    "\n",
    "print(\"-----------------Classification Report-----------------\")\n",
    "print(f\"{classification_report(whole_y_test, y_pred,target_names = newsgroups_train['target_names'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c604e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Classification Report(subset)-------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  alt.atheism       0.93      0.88      0.90        48\n",
      "comp.graphics       0.86      0.89      0.88        55\n",
      "    sci.space       0.86      0.87      0.86        62\n",
      "\n",
      "     accuracy                           0.88       165\n",
      "    macro avg       0.88      0.88      0.88       165\n",
      " weighted avg       0.88      0.88      0.88       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using subset(in order to compare with the third exercise)\n",
    "vectorizer2 = CountVectorizer()\n",
    "train_instances_countmatrix = vectorizer.fit_transform(train_instances)\n",
    "test_instances_countmatrix = vectorizer.transform(test_instances)\n",
    "\n",
    "lr_model_sub = LogisticRegression(random_state=442).fit(train_instances_countmatrix, y_train)\n",
    "y_pred2 = lr_model_sub.predict(test_instances_countmatrix)\n",
    "\n",
    "print(\"-------------Classification Report(subset)-------------\")\n",
    "print(f\"{classification_report(y_test, y_pred2,target_names = newsgroups_train['target_names'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90078982-d0ec-4881-9bc2-afa6b46101b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Classification Report-----------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  alt.atheism       0.98      0.92      0.95       319\n",
      "comp.graphics       0.91      0.96      0.93       389\n",
      "    sci.space       0.95      0.94      0.95       394\n",
      "\n",
      "     accuracy                           0.94      1102\n",
      "    macro avg       0.94      0.94      0.94      1102\n",
      " weighted avg       0.94      0.94      0.94      1102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#normalize the frequencies with the inverse document frequency\n",
    "#using the whole train and test set\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "whole_train_tfidf = tfidf_vec.fit_transform(whole_train_instances)\n",
    "whole_test_tfidf = tfidf_vec.transform(whole_test_instances)\n",
    "\n",
    "\n",
    "lr_model2 = LogisticRegression(random_state=442).fit(whole_train_tfidf, whole_y_train)\n",
    "y_pred_tfidf = lr_model2.predict(whole_test_tfidf)\n",
    "\n",
    "print(\"-----------------Classification Report-----------------\")\n",
    "print(f\"{classification_report(whole_y_test, y_pred_tfidf,target_names = newsgroups_train['target_names'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a37f3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Classification Report(subset)-------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  alt.atheism       0.96      0.94      0.95        48\n",
      "comp.graphics       0.92      0.89      0.91        55\n",
      "    sci.space       0.91      0.95      0.93        62\n",
      "\n",
      "     accuracy                           0.93       165\n",
      "    macro avg       0.93      0.93      0.93       165\n",
      " weighted avg       0.93      0.93      0.93       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using subset\n",
    "tfidf_vec2 = TfidfVectorizer()\n",
    "train_instances_tfidf = tfidf_vec2.fit_transform(train_instances)\n",
    "test_instances_tfidf = tfidf_vec2.transform(test_instances)\n",
    "\n",
    "lr_model_sub2 = LogisticRegression(random_state=442).fit(train_instances_tfidf, y_train)\n",
    "y_pred_tfidf2 = lr_model_sub2.predict(test_instances_tfidf)\n",
    "\n",
    "print(\"-------------Classification Report(subset)-------------\")\n",
    "print(f\"{classification_report(y_test, y_pred_tfidf2,target_names = newsgroups_train['target_names'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967220f-dad9-4117-83c1-38d093c02b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Classification Report-----------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  alt.atheism       0.93      0.90      0.91        48\n",
      "comp.graphics       0.86      0.89      0.88        55\n",
      "    sci.space       0.87      0.87      0.87        62\n",
      "\n",
      "     accuracy                           0.88       165\n",
      "    macro avg       0.89      0.89      0.89       165\n",
      " weighted avg       0.89      0.88      0.89       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#extract named entities and check whether this improve results\n",
    "import spacy\n",
    "model = spacy.load('en_core_web_sm')\n",
    "\n",
    "def gen_named_entity(text):\n",
    "    doc = model(text)\n",
    "    name_entity = [ent.text.replace(\" \",\"_\") for ent in doc.ents if len(ent.text.split())>1]\n",
    "    return \" \".join(name_entity)\n",
    "\n",
    "train_instances_processed = [doc + \" \" + gen_named_entity(doc) for doc in train_instances]\n",
    "test_instances_processed = [doc + \" \" + gen_named_entity(doc) for doc in test_instances]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "train_instances_vec = vec.fit_transform(train_instances_processed)\n",
    "test_instances_vec = vec.transform(test_instances_processed)\n",
    "\n",
    "lr_model3 = LogisticRegression(random_state=442).fit(train_instances_vec, y_train)\n",
    "y_pred_vec = lr_model3.predict(test_instances_vec)\n",
    "\n",
    "print(\"-----------------Classification Report-----------------\")\n",
    "print(f\"{classification_report(y_test, y_pred_vec,target_names = newsgroups_train['target_names'])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
