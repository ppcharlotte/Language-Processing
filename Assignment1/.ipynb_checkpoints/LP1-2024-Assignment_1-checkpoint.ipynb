{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6706ccb",
   "metadata": {},
   "source": [
    "# Language Processing 1 (fall 2024)\n",
    "\n",
    "## Assignment 1: assigned on September 30. 2024  and  to be returned on October 23, 2024, 23:55.\n",
    "\n",
    "#### The percentages assigned to the exercises are provided to give you an idea of the time/effort it might take to solve them.  \n",
    "\n",
    "#### You must have at least 70% correct to pass the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b76f7b",
   "metadata": {},
   "source": [
    "## Exercise 1 (25%):\n",
    "\n",
    "- Download the ELTeC corpora in English, Spanish, and another language of interest of your choice (hopefully one that you do not speak) from https://zenodo.org/records/4662444. If there are more versions of a language corpus choose the smallest one (level0).\n",
    "  - English corpus: https://zenodo.org/records/4662490\n",
    "  - Spanish corpus: https://zenodo.org/records/4662603\n",
    "  - Other corpora: Check \"Related works\" in https://zenodo.org/records/4662444\n",
    "- Read the three corpora (NB: you can use a Python package to read TEI files, e.g. [tei-reader](https://pypi.org/project/tei-reader/))\n",
    "- Export a raw text corpus of English, Spanish, and the third language you have chosen (called 3-L henceforth).\n",
    "  - You should save the output in raw text format in three files: en.txt, sp.txt and xx.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c03e182-a8b7-4a23-ab32-3c8e431c2b60",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tei_reader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Insert your code below:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtei_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TeiReader\n\u001b[1;32m      3\u001b[0m reader \u001b[38;5;241m=\u001b[39m TeiReader()\n\u001b[1;32m      4\u001b[0m corpora \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mread_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mENG18450_Disraeli\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tei_reader'"
     ]
    }
   ],
   "source": [
    "# Insert your code below:\n",
    "from tei_reader import TeiReader\n",
    "reader = TeiReader()\n",
    "corpora = reader.read_file('ENG18450_Disraeli')\n",
    "print(corpora.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6850099d",
   "metadata": {},
   "source": [
    "# Exercise 2  (10%):\n",
    "- Write a tokenizer for the three languages (English, Spanish, and 3-L), based on regexps.\n",
    "- Print the first 100 tokens for each language.\n",
    "- Save in a file \"EN_mytok_red.txt\" the first 2000 tokenized words in English.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f479d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f134f78",
   "metadata": {},
   "source": [
    " # Exercise 3 (10%)\n",
    "- Use the NLTK tokenizer and run it on the English corpus.\n",
    "- Write to the file \"EN_NLTK_tok.txt\" the first 2000 tokens.\n",
    "- Compare the content of \"EN_mytok_red.txt\" and \"EN_NLTK_tok.txt\" and explain whether and how they differ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f741d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fda8ba-2640-4d02-b8da-e244e580c52b",
   "metadata": {},
   "source": [
    "## Your explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605076e",
   "metadata": {},
   "source": [
    "# Exercise 4 (10%)\n",
    "- Stem the English corpus using the NLTK Porter stemmer.\n",
    "- Lemmatize the English corpus using the NLTK WordNetLemmatizer.\n",
    "\n",
    "- Explain the main differences looking at the first 50 elements (stems vs. lemmas) they produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1192b920-351e-4efa-827c-0d7d227340a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3756b0-bd04-4b49-b1ce-d722c384fdff",
   "metadata": {},
   "source": [
    "### Your Explanation here (press enter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10550e10",
   "metadata": {},
   "source": [
    "# Exercise 5 (20%)\n",
    "- Plot the frequency distribution and the zipf distribution for the three corpora (English, Spanish, 3-L).\n",
    "- Explain what they show to you.\n",
    "- Try to guess how to say \"and\" in the unknown language (3-L), without using any translation service or dictionary. How did you guess it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7312517-d1cc-4d6f-85ed-843f118f35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c169224-310f-40b0-a338-d68494bde432",
   "metadata": {},
   "source": [
    "## Your explanations here (press enter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f55bd6-b62c-4193-97c8-e5707013537d",
   "metadata": {},
   "source": [
    "# Exercise 6: 10%\n",
    "\n",
    "For the English corpus:\n",
    "- Sentence tokenize it via NLTK.\n",
    "- Remove the stop words (for the English corpus only).\n",
    "- Print the first 10 sentences.\n",
    "- Write the pre_processed corpus to the file \"EN_preproc.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd63fd-c4f3-4253-8e47-623523fa58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3206d2a6-05dc-467b-8428-ac497c79bea1",
   "metadata": {},
   "source": [
    "# Exercise 7: 5%\n",
    "- Explain what is normalization of text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919a2c4-a92d-4feb-8a1e-bb135799e047",
   "metadata": {},
   "source": [
    "## Your answer here (press enter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77204b1b-3b9f-45d8-a9db-89fefe910c52",
   "metadata": {},
   "source": [
    "# Exercise 8 (10%)\n",
    "- Run the Universal PoS-tagger on the tokenized English text and print how many substantives there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6db54b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278f6f7-4812-4af5-87e7-8d23ec0305a1",
   "metadata": {},
   "source": [
    "### The end!\n",
    "\n",
    "Thanks for doing this assignment. We hope you enjoyed it. In the following assignments we will use the same corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
